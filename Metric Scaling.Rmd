---
title: "Metric Scaling"
author: "Eveline Surbakti - 19200629"
date: "3/31/2020"
output: html_document
---

### 2 (a) Apply classical metric scaling to the data, explaining any decisions you make in the process. 
```{r}
# Import the data.
hyptis<-read.csv("Hyptis.csv")
# summary(hyptis)
# Get all numerical data 
# decided to not scale at the moment because they are the concentration and measured using the terpene measures
hyptis_data<-(hyptis[,-8])
```
Choose a suitable number of dimensions required to represent the resulting configuration. Explain, using a graphic to support your argument, your reasoning behind your choice of the required number of dimensions. 

__Answer__:
We can see from the data, it only has 30 observations, this is quite small and we need to represent the resulting configuration. 
```{r}
library(mclust)
# generate the cmdscale based on the different k
loc1 = cmdscale(dist(hyptis_data), k=1, eig=TRUE)
loc2 = cmdscale(dist(hyptis_data), k=2, eig=TRUE)
loc3 = cmdscale(dist(hyptis_data), k=3, eig=TRUE)
loc4 = cmdscale(dist(hyptis_data), k=4, eig=TRUE)
loc5 = cmdscale(dist(hyptis_data), k=5, eig=TRUE)
loc6 = cmdscale(dist(hyptis_data), k=6, eig=TRUE)
loc7 = cmdscale(dist(hyptis_data), k=7, eig=TRUE)

# proportion of variation
x<- c(sum(abs(loc1$eig[1:1]))/sum(abs(loc1$eig)),
sum(abs(loc2$eig[1:2]))/sum(abs(loc2$eig)),
sum(abs(loc3$eig[1:3]))/sum(abs(loc3$eig)),
sum(abs(loc4$eig[1:4]))/sum(abs(loc4$eig)),
sum(abs(loc5$eig[1:5]))/sum(abs(loc5$eig)),
sum(abs(loc6$eig[1:6]))/sum(abs(loc6$eig)),
sum(abs(loc7$eig[1:7]))/sum(abs(loc7$eig)))

plot(x ,type = "l", ylab = "Proportion of Variation", main ="Optimal Value for q (Classical)")
points(4,sum(abs(loc4$eig[1:4]))/sum(abs(loc4$eig)), pch = 19, col = adjustcolor("darkorange2", 0.5))

# 4 dimension is the best answer! to avoid the overfitting and get enough representation of the data
```

I provided k up to 7 dimensions to see the proportionality of variation. It is known that the best k is the one that give the greater representation of the data. Let us examine, in every visualization, two dimension will always give a great representation as the start and in this case it give more than 70% proportion of variation which is still acceptable. It gets higher as we increase the dimensions. 

In this case, With 90% of proportion of variation, for 30 observations which is very small size population, 4 dimensions can be chosen as the ideal setting to represent the data. The higher value than 4 will give a higher proportion of variation but we also want to avoid overfitting as it may not generalise well.

Plot the two dimensional configuration resulting from the application of classical metric scaling. Label each point in your plot using the geographical location of each plant.

__Answer__:
```{r}
# Code the geographical location of each plant.
S = which(hyptis[,8] == "South")
N = which(hyptis[,8] == "North")
EH = which(hyptis[,8] == "East-high")
EL = which(hyptis[,8] == "East-low")

x2=loc2$points[,1]
y2=loc2$points[,2]
cols=c(rep(1,10),rep(2,9),rep(3,6),rep(4,5))
plot(x2, y2, type="n", xlab="", ylab="", main="Classical-2")
text(x2, y2, hyptis[c(S,N,EH,EL),8], cex=1, col=cols)
```

The geographical location with different colors in the graph show us the clustering for the observations. It is well present despite of a few observations may located in other cluster area. 

### 2 (b) Apply Sammon's metric least squares scaling and Kruskal's non-metric scaling to the data. 

__Answer__:
```{r}
library(MASS)
# SAMMON
# generate the sammon based on the different k
loc11 = sammon(dist(hyptis_data), k=1)
loc21 = sammon(dist(hyptis_data), k=2)
loc31 = sammon(dist(hyptis_data), k=3)
loc41 = sammon(dist(hyptis_data), k=4)
loc51 = sammon(dist(hyptis_data), k=5)
loc61 = sammon(dist(hyptis_data), k=6)
loc71 = sammon(dist(hyptis_data), k=7)

# the stress 
x1<- c(loc11$stress,loc21$stress,loc31$stress,loc41$stress,loc51$stress
       ,loc61$stress,loc71$stress)

# minimizing the stress
plot(x1 ,type = "l", ylab = "Stress", main ="Optimal Value for q (Sammon)")
points(4,loc41$stress, pch = 19, col = adjustcolor("darkorange2", 0.5))

# 4 dimension is the best answer! to avoid the overfitting and get enough representation of the data while minimizing the stress
```

```{r}
# Kruskal
# generate the isoMDS based on the different k
loc12 = isoMDS(dist(hyptis_data), k=1)
loc22 = isoMDS(dist(hyptis_data), k=2)
loc32 = isoMDS(dist(hyptis_data), k=3)
loc42 = isoMDS(dist(hyptis_data), k=4)
loc52 = isoMDS(dist(hyptis_data), k=5)
loc62 = isoMDS(dist(hyptis_data), k=6)
loc72 = isoMDS(dist(hyptis_data), k=7)

# the stress 
x2<- c(loc12$stress,loc22$stress,loc32$stress,loc42$stress,loc52$stress
       ,loc62$stress,loc72$stress)

# minimizing the stress
plot(x2 ,type = "l", ylab = "Stress", main ="Optimal Value for q (Kruskal)")
points(4,loc42$stress, pch = 19, col = adjustcolor("darkorange2", 0.5))

# 4 dimension is the best answer! to avoid the overfitting and get enough representation of the data while minimizing the stress
```

Overlay the resulting two dimensional configurations on your plot of the classical scaling configuration. Label each point using the observation number of the associated plant.

__Answer__:
The graphs show that the scale for both is different, we can see that both graphs look pretty similar despite of few observations have the different location. For example, we can see the observation 15 and 16 in both graphs, while the metric least square scaling shows that both observations are located closer to observation 14 but in the Kruskal's non metric scaling, both observations now closer to observation 19. 

```{r}
x21 = loc11$points[,1]
y21 = loc21$points[,2]
x22 = loc12$points[,1]
y22 = loc22$points[,2]

par(mfrow=c(1,2))
plot(x21, y21, type="n", xlab="", ylab="",main="Metric least squares scaling")
text(x21, y21, #hyptis[c(S,N,EH,EL),8], 
     cex=1, col=cols)
plot(x22, y22, type="n", xlab="", ylab="", main="Kruskal's non-metric scaling")
text(x22, y22, #hyptis[c(S,N,EH,EL),8], 
     cex=1, col=cols)
```

### 2(c) Use Procrustes analysis to match the three resulting configurations of the plants. 

__Answer__:
I use four dimensional in this analysis. 

```{r}
 #install.packages("vegan")
 library(vegan)
 proc12 = procrustes(loc4$points, loc41$points)
 proc23 = procrustes(loc41$points, loc42$points)
 proc31 = procrustes(loc42$points, loc4$points)
 
 par(mfrow=c(2,3))
 plot(proc12, main = "Classical vs Sammon")
 plot(proc23, main = "Sammon vs Kruskal")
 plot(proc31, main= "Kruskal vs Classical")
 plot(proc12, kind=2, main = "Classical vs Sammon")
 plot(proc23, kind=2, main = "Sammon vs Kruskal")
 plot(proc31, kind=2, main= "Kruskal vs Classical")
```

Function Procrustes rotates a configuration to maximum similarity with another configuration. Let us examine the comparison between the Classical, Sammon and Kruskal configuration. The first row graphs are ordination diagrams while the second row is the residuals between both configurations.

In the first row, all of the graphs show that all of the observations are located almost in a similar location and so do the arrows head directions. For the second row, the residuals between the configuration are distinct from one another especially the scale. 

Which configurations match best? Suggest a reason for your conclusion.
__Answer__:
So far, among the comparisons, Kruskal vs Classical is very similar with most of the residuals below 0.9, the configurations match best. It supposed to happen because isoMDS function originally will set its y (as a part of the function) as an initial configuration. And since none is supplied, cmdscale is used (as y) to provide the classical solution, unless there are missing or infinite dissimilarities. 

### 2 (d) Perform model-based clustering on these data. 

Model-based clustering based on parameterized finite Gaussian mixture models. Models are estimated by EM algorithm initialized by hierarchical model-based agglomerative clustering. The optimal model is then selected according to BIC.

```{r}
# Clustering the data, checking the pairs plot first
library(mclust)
# we can visualize the data with this plot
pairs(hyptis_data)
```

```{r}
res = Mclust(hyptis_data, G=1:7, verbose = FALSE)
summary(res)
```

```{r}
#will give us the BIC and top 3 models
res$BIC 
plot(res, what="BIC", legendArgs = list(x="topleft", cex=0.5,horiz=T),ylim=c(-1600, -1100))
```

The optimal model is often chosen using the Bayesian information criterion (BIC): This criterion is calculated for each different type of model fitted and the model which has the largest value of this criterion is deemed to be the optimal model.

How many clusters are chosen as optimal? Explain how you arrived at your answer.

__Answer__:
The optimal model for hyptis data chosen by BIC is VEV, with G=6 clusters. VEV's BIC is -408.9115, the clusters are ellipsoidal, have varying volume,varying shape and varying orientation.

Here is the resulting clusters. 

```{r}
plot(res, what="classification")
```


Here is the clustering uncertainty. 
```{r}
plot(res, what="uncertainty")

```


And the clusters size:
```{r}
table(res$classification) # see cluster size
```
